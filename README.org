* Documentation
  For now, the only way to obtain documentation is to generate it.
  You can only generate it once the project is installed (Sphinx
  load Python modules in order to generate documentation, but,
  unless the project is installed, the imports will not work).

  The way to do this is to:

  #+BEGIN_SRC sh
    virtualenv ./.venv
    ./.venv/bin/activate
    ./setup.py install
    ./setup.py build_sphinx
  #+END_SRC

* About
  This project is an attempt to re-implement Protocol Buffers parser
  (both sources and binary) as a C extension for Python.

  Only Version 3 of Protocol Buffers is and will be supported.  Only
  Python 3 is and possibly future version will be supported.

  Unfortunately, =protoc=, the official implementation of Protocol
  Buffers accepts a lot more than the specification allows, in
  particular, it accepts virtually any Protobuf v2 syntax in Protobuf
  v3 source files.  This implementation tries to match this behavior.

** Note
   Imports are resolved differently in this implementation and in
   =protoc=: this implementation allows parsing source files in
   parallel, however, this resolves certain edge cases of =import=
   statements differently from how =protoc= does it.  This is as
   intended.

** What's Different?
   Protobuf was initially written for C++, where run-time code
   generation is problematic.  When C++ approach was copied into other
   languages, it may or may not have made sense there.  In particular,
   in Python, having generated Python code for parsing messages
   doesn't make much sense.  This is why this implementation doesn't
   create descriptors and Python sources doing the deserialization.
   All you need is to point the parser to the Protobuf source files.

** Some other differences
   - Options are parsed but ignored.  Options are needed for =protoc=
     plugins to do pre- or post-processing on generated sources.
     =protopy= works in a very different way, so that none of =protoc=
     plugins would even make sense here.
   - Options on fields (such as =[default true]= on =bool= fields) are
     ignored.  The behavior of fields that weren't sent over the wire
     is different: they will be always set to =None=.  There's no
     concept of default values.  This is as intended.
   - Fields sent over the wire, which are not in the description are
     silently ignored by =protoc= generated code, but result in errors
     in =protopy=.  In general, I found the later to be a saner
     approach.  In the future, I might consider making this
     configurable.
   - =protopy= doesn't attempt to find errors in your definitions.  It
     may allow things that aren't valid for =protoc=.  If you want a
     better error reporting, =protoc= is still a better tool.  One
     particular difference here is how definitions get resolved to
     their full names.  It is technically possible to create an
     ambiguous name using Protobuf IDL: this is because it is not
     possible to tell which part of the name represents the package
     and which part represents the local name.  Thus:
     : foo.bar.baz
     : ^   ^--- local name: "bar.baz"
     : ^------- package: "foo"
     or
     : foo.bar.baz
     : ^       ^-- local name: "baz"
     : ^---------- package: "foo.bar"
     =protoc= will actually alert you when you have this kind of
     conflict.  =protopy= will use whichever definition it encountered
     last.
   - Zeroth enumeration values: my reading of documentation is that
     #+BEGIN_SRC protobuf
       enum Foo {
           BAR = 0;
           BAZ = 1;
       }
     #+END_SRC
     Shouldn't be even valid.  But =protoc= accepts this.  Actually,
     it requires enums to have at least one member, and at least one
     member whose value is zero.  =protopy= makes no distinction
     between zeroth member of an enum and other indices, but since
     =protoc= assigns a special meaning to zeroth member: it will use
     that as a default, if no value is given for the field, you should
     always have zeroth member of every enumeration to stay
     compatible.

** GRPC
   I have no plans supporting this feature, =service= and =rpc=
   definitions are accepted by the parser but result in empty nodes.

** Benchmarks
   Planned, but haven't been done yet.

** Serializing
   The approach taken by the library differs from how =protobuf=
   module works.  =protobuf= is designed in a way, perhaps, suitable
   for C++ or Java language, but it is foreign to Python on a
   conceptual level.  Python's actual type-system is /structural/ in
   its heart.  This means that types in Python are defined in terms of
   type operations like addition and multiplication on the basic
   constituents.  Thus, given:
   #+BEGIN_SRC python
     from collections import namedtuple

     class Foo:
         def __init__(self, x, y):
             self.x = x
             self.y = y

     Bar = namedtuple('Bar', 'x, y')

     def baz(arg):
         print('arg: {} {}'.format(arg.x, arg.y))
   #+END_SRC

   =Foo(1, True)= is of the same type as =Bar(1, True)= from a
   standpoint of =baz=.

   This is while Java type system is /nominative/ in its heart.  Types
   are almost never considered the same, unless they are defined to be
   interchangeable intentionally in the source of the program.

   Since serialization operates on a multi-program level, there is no
   real way for environments like Java or C++ to ensure type
   correctness of serialized data.  And this is why, I believe, the
   approach taken by this library is better.  =protopy.Serializer=
   doesn't try to establish that the serialized object belongs to a
   specific class.  The requirements for object to be serialized as a
   particular message are that it responds to the very broadly defined
   protocols.  In particular, in order for the object to be serialized
   as a message, it needs to be iterable.  The iteration must provide
   mapping between field values and their desired keys.

   Similarly, if the field is encoded as =repeatable= its value must
   implement [[https://docs.python.org/3/c-api/sequence.html][sequence protocol]].  If it maps to a =map= field, then it
   must implement [[https://docs.python.org/3/c-api/mapping.html][mapping protocol]].  Atomic values must implement
   corresponding protocols too.  I.e. if a field is to be encoded as
   =int32=, then it should be either an integer, or an object whose
   class inherits from integer, or it must define =__int__()= method.
   If it is meant to be encoded as a string, it must define
   =__str__()= and so on.

   This also means that, unlike in =protobuf= package, there are no
   special message classes that implement =MessageToString()= or
   =MessageFromString()= methods etc.  I believe that this is a bad
   way to go about serialization: it encourages boilerplate code that
   translates between domain objects into objects whose only purpose
   is to be accepted by serializer before they are discarded.  This is
   just wasteful.

   Unlike when using =protobuf= package, =protopy= can serialize
   atomic values s.a. integers and strings.  This removes the need for
   proprietary extensions s.a. [[https://github.com/google/protobuf/blob/master/src/google/protobuf/wrappers.proto#L107][=goolge.protobuf.wrappers.StringValue=]].
   
** Customizing (de)serialization
   Sometimes you may encounter Protobuf messages with special
   serialization behavior.  For example, the =StringValue= mentioned
   above is intended to map to a =str= instances in Python.  =protopy=
   doesn't do this translation automatically.  Instead, it allows you
   to substitute known message types with your own constructors.

   For example, you could do this:

   #+BEGIN_SRC python
     content = ...
     parser = BinParser(['.'])

     result = parser.parse('test.proto', 'Wrapper', content)
     original = parser.def_parser.find_definition(b'Wrapper')

     class Replacement:

         def __init__(self, original):
             self.original = original

         def replacement(self, *args):
             return {'replaced': self.original(*args)}

     rp = Replacement(original)
     parser.def_parser.update_definition(b'Wrapper', rp.replacement)

     result = parser.parse('test.proto', 'Wrapper', content)
     assert result['replaced'].wrapped_type == 'Wrapped'
   #+END_SRC
   
   However, you should be extremely careful when doing this.  The
   above example will prevent you from serializing the resulting
   message back into it's binary form.

** Speedups
   Python enumerators are very complex and slow.  Yet, enumerations
   are one of the basic building blocks of Protobuf definitions.
   =DefParser= allows you to customize the way enumerations are
   instantiated when binary payload is read.  In particular, there's
   even a helper procedure, that you can substitute in place of the
   default enumeration factory: =protopy.parser.simple_enum= in order
   to speed up parsing.  Note that this function will make enumerators
   impossible to serialize back.

* Progress
  Many parts of the code are still of prototype quality, however, the
  interface is more or less stable.

** Priority Tasks
   - [X] Memory (de)allocation needs to be:
     1. Done from APR pools.
     2. More fine-grained pools.
   - [ ] Naming needs work, some names use inconsistent conventions.
   - [ ] Const correctness.  A lot of code lacks this.
   - [X] Revise how arguments are supplied to message constructors, maybe
     we can shave some fat there by creating a tuple right away rather
     than collecting them into a hash-table and then into a tuple.
   - [X] Rewrite =setup.py= so that it also builds the lexer and the
     parser (maybe, conditionally), then get rid of =main.c= and few
     more junk files in =lib=.
   - [X] Few more exotic types need testing: very long varints and floats,
     I think they don't parse correctly.
   - [X] =defparser= is kind of a mess, it can be reorganized and
     cleaned up a bit.
   - [X] ints in =list= could be encoded into pointers instead of
     allocating extra memory.
   - [X] =cons= may have an alternative version, where it doesn't
     allocate more memory, but uses the the =value= as is.
     /Irrelevant since using APR pools/.
   - [X] Some code in =protopy.y= never releases memory / could
     allocate less.
     /Irrelevant since using APR pools/.
   - [ ] Serializer needs work, a lot of functionality there repeats,
     and may be consolidated.
   - [ ] It seems like there's a bug with scheduling of parsing files,
     somehow very few threads get scheduled when reading files in
     bulks.

** Medium priority
   Keep this number low
   #+BEGIN_SRC sh
     find ./protopy \( -name '*.py' -or -name '*.[chyl]' \) -exec wc -l {} +
   #+END_SRC

   Right now it's 11315, I would like to get in under 10K.
* License
  Finally, I was able to put this project under a free license.

  This project is licensed under LGPL v3.  It relies on Apache
  Portable Runtime library, which is licensed under Apache 2.0 license
  (find the license text under [[lib]] directory.)
